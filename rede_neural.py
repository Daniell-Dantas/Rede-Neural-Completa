# -*- coding: utf-8 -*-
"""Rede Neural.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GLDHoyAds4tr2RLbR5kgthgn7EbJfXQd

#Configurações iniciais

**Import de funções necessárias**
"""

import matplotlib.pyplot as plt 
import seaborn as sns
import numpy as np
import pandas as pd
import random
import math
from sklearn.metrics import confusion_matrix
import seaborn as sns

"""**Leitura dos dados do arquivo de mamografia**"""

#LEITURA DOS DADOS

dados = pd.read_csv('./dadosmamografia.csv', delimiter = ",", names=['1','2','3','4','5','6'])
dados = dados.transpose() # Carregar os padrões de treino

"""**Divisão dos dados**"""

# DIVISÃO DOS DADOS

treino = dados.iloc[:, 0:502] #60% dados de treino
I_treino = treino.iloc[:5, :] #Definição do vetor de entrada para ser normalizado
Saida_desejada_treinamento = treino.iloc[5:6, :]

teste = dados.iloc[:, 502:668] #20% dados de teste
I_teste = teste.iloc[:5, :] #Definição do vetor de entrada para ser normalizado
Saida_desejada_teste = teste.iloc[5:6, :]

validacao = dados.iloc[:, 668:834] #20% dados de validação
I_validacao = validacao.iloc[:5, :] #Definição do vetor de entrada para ser normalizado
Saida_desejada_validacao = validacao.iloc[5:6, :]

"""#Requisições ao usuário

**Parâmetros**
"""

# REQUISIÇÕES AO USUÁRIO E PARÂMETROS

Neuronios_oculta=int(input('Nerônios na Camada Oculta: ')) #Pedir o número de neuronios da camada oculta
Neuronios_saida = 1; #Especificar o número de neuronios da camada de saída, no nosso caso é 1
epocas = int(input('Digite o número de épocas: ')) #Pedir o número de épocas
taxa_aprendizagem = float(input('Digite a taxa de aprendizagem: ')) #Pedir a taxa de aprendizagem
E = math.exp(-10); #Definir precisão (limite de erro)

#@title Selecione a função de ativação da camada escondida { run: "auto" }

fun_ativacao_input = 'sigmoide' #@param ["sigmoide", "linear", "tangente_hiperbolica"]
print("Foi escolhido: " + fun_ativacao_input)

# DEFINIÇÃO DAS FUNÇÕES DE ATIVAÇÃO E DERIVADAS

def sigmoide(arg):
  resultado = 1/(1 + math.exp(-arg))
  return resultado

def deriv_sigmoide(arg):
  
  resultado = (math.exp(arg) / (1 + math.exp(arg)) ** 2)
  return resultado

def linear(arg):
  resultado = arg
  return resultado

def deriv_linear(arg):
  resultado = 1
  return resultado

def tangente_hiperbolica(arg):
  resultado = (1 - math.exp(-arg))/(1 + math.exp(-arg))
  return resultado

def deriv_tangente_hiperbolica(arg):
  resultado = (2*math.exp(-arg))/((math.exp(-arg) + 1) ** 2)
  return resultado
  
# Salvando a função de ativação escolhida pelo usuário:

if (fun_ativacao_input == 'sigmoide'):
  fun_ativacao = sigmoide
  derivada = deriv_sigmoide
elif (fun_ativacao_input == 'linear'):
  fun_ativacao = linear
  derivada = deriv_linear
else:
  fun_ativacao = tangente_hiperbolica
  derivada = deriv_tangente_hiperbolica

"""#Manipulação e configurações de dados e variáveis

**Normalização de dados**
"""

# DEFINIÇÃO DA FUNÇÃO DE NORMALIZAÇÃO
def normalizacao(entrada):
  # Normalização do treinamento
  min = entrada.min().min() # Valor mínimo
  max = entrada.max().max() # Valor máximo
  normalizado = ((entrada - min)/(max - min)) #considerando Lmax = 1 e Lmin = 0.
  return normalizado


# Normalização do treinamento
normalizado_treino = normalizacao(I_treino) 
Entrada_treinamento = normalizado_treino

# Normalização do teste
normalizado_teste = normalizacao(I_teste)
Entrada_teste = normalizado_teste

# Normalização da validação
normalizado_validacao = normalizacao(I_validacao)
Entrada_validacao = normalizado_validacao

"""**Definição de índice de matrizes a serem usadas**"""

# ORGANIZANDO AS MATRIZES - Atribuindo os valores dos índices das matrizes

[numero_linhas_treinamento, numero_colunas_treinamento] = Entrada_treinamento.shape #Define o número de linhas e colunas da matriz de entrada no treinamento. Precisamos desses valores para fazer alógica do Foward.
[numero_linhas_teste, numero_colunas_teste] = Entrada_teste.shape #Define o número de linhas e colunas da matriz de entrada no teste
[numero_linhas_validacao, numero_colunas_validacao] = Entrada_validacao.shape #Define o número de linhas e colunas da matriz de entrada na validação

"""**Inicialização randomica de pesos & Criação de variáveis de acordo com as dimensões adotads**"""

# Inicializar os valores dos pesos da camada 1 e 2
w1 = np.random.random([Neuronios_oculta,numero_linhas_treinamento+1]) #PESOS DA CAMADA OCULTA 
w2 = np.random.random([Neuronios_saida,Neuronios_oculta+1]) #PESOS DA CAMADA DE SAÍDA

#Definimos os novos pesos ww1 e ww2 para poder guardar o valor dos pesos originais e mostrar os pesos finais
ww1=w1 
ww2=w2

#Criar as variáveis que vão guardar os valores
Saida_treinamento = pd.DataFrame(np.zeros((1, numero_colunas_treinamento))) #criando a saída do treinamento
Saida_teste = pd.DataFrame(np.zeros((1, numero_colunas_teste))) #criando a saída do treinamento


Saida_treinamento_total = pd.DataFrame(np.zeros((epocas, treino.shape[1]))) #criando a saída do tamanho da base de treinamento

EQM_train = pd.DataFrame(np.zeros((1, epocas))) #criando o dataframe que guardará os valores do erro médio quadrático
ERRO_validacao = pd.DataFrame(np.zeros((1, epocas))) #criando o dataframe que guardará o valor do erro de validação

"""# Definição de funções: As células a seguir contem as funções que serão usadas na célula principal para fazer as funções da rede neural

**Neurônio artificial**
"""

#DEFININDO A FUNÇÃO QUE FARÁ O PAPEL DO NEURÔNIO ARTIFICIAL

def neuronio(ww1,ww2,dado):

    # CAMADA DE ENTRADA
    x = pd.concat([pd.Series(-1), dado]).to_numpy() # concatenando o -1 do bias com os valores de entrada do treinamento
    Ij1 = ww1.dot(x) #Multiplicação matricial
    fun_ativacao_v = np.vectorize(fun_ativacao) # vetorizando a função
    Yj1 = fun_ativacao_v(Ij1) #Função de ativação da camada 1 (oculta): O usuário escolheu a função de ativação. Esta é a saída dos neurônios da camada oculta
      
    
    # CAMADA DE SAÍDA
    y = np.concatenate(([-1], Yj1)) # Aqui é a saída dos neurônios da camada oculta + bias. 
    Ij2 = ww2.dot(y) # Entradas para o neurônio da camada de saída - Multiplicação matricial
    linear_v = np.vectorize(linear) # vetorizando a função
    Yj2 = linear_v(Ij2) # Função de ativação da camada de saída: Foi escolhida a função Linear. Esta é a saída do neurônio da camada de saída, e portanto, da rede

    return Yj2, x, Ij1, Yj1, y, Ij2

"""**Validação Cruzada**"""

# DEFININDO A FUNÇÃO QUE FARÁ A VALIDAÇÃO CRUZADA

def validacao_cruzada(epoca,criterio_treino,criterio_valid):
  contador = 0
  erro_validacao = [0]
  


  if (epoca % (round(numero_colunas_treinamento / numero_colunas_validacao)+1) == 0): #executamos a validação a cada 4 iterações
    saida_validacao = neuronio(ww1,ww2,Entrada_validacao.iloc[:, contador])
    erro_validacao = Saida_desejada_validacao.iloc[:, contador] - saida_validacao[0] # Sinal de erro da camada de saída da validação
    ERRO_validacao.iloc[0, epoca] = erro_validacao[0]
    contador = contador + 1
  elif (epoca>0):
    ERRO_validacao.iloc[0, epoca] = ERRO_validacao.iloc[0, epoca-1]


  if(epoca>1):
    criterio_treino = EQM_train.iloc[0, epoca-1] > EQM_train.iloc[0, epoca]
    if (ERRO_validacao.iloc[0, epoca-1] < ERRO_validacao.iloc[0, epoca]) and (criterio_treino == True):
      criterio_valid = criterio_valid + 1
    
  
  print('na época ' + str(epoca) + ' temos o criterio_valid ' + str(criterio_valid) + ' e o erro de validação de {:.5f}'.format(ERRO_validacao.iloc[0,epoca]))
  print('na época ' + str(epoca) + ' temos o criterio_treino ' + str(criterio_treino) + ' e o erro de treino de {:.5f}'.format(EQM_train.iloc[0,epoca]))

  return criterio_treino,criterio_valid

"""**Ajuste dos pesos**"""

#DEFININDO A FUNÇÃO QUE FARÁ O AJUSTE DOS PESOS

def ajuste_pesos(epoca,ww1,ww2):

  erro = [0] # Declarando variável
  Erro_quadratico=0 # Declarando variável

  for i in range(numero_colunas_treinamento): #Loop que faz iterações do tamanho da quantidade de colunas de treinamento, ou seja, número de pares de entrada no banco de dados

    saida_neuronio, x, Ij1, Yj1, y, Ij2 = neuronio(ww1,ww2,Entrada_treinamento.iloc[:, i]) # Passamos os dados de entrada pelo neurônio artificial
    erro = Saida_desejada_treinamento.iloc[:, i] - saida_neuronio # Sinal de erro da camada de saída do treinamento
    

    # INÍCIO BACKWARD
    Erro_quadratico = Erro_quadratico + (0.5*sum(erro ** 2)) # Considerando todos os neurônios da camada de saída, a soma do erro quadrático no instante N, ou seja, para um par entrada saída. No nosso caso, a soma terá o valor de somente 1 erro.
    derivada_saida_rede = deriv_linear(Ij2) #Derivada da função linear na camada de saída
    gradiente_local_saida = erro*derivada_saida_rede #Gradiente local da camada de saida.

    # Ajuste dos pesos na camada de saída
    ww2 = ww2 + (y * (taxa_aprendizagem*gradiente_local_saida)[0])

    # Ajuste dos pesos na camada oculta
    derivada_v = np.vectorize(derivada) # vetorizando a função derivada sigmoide
    derivada_saida_camada_oculta = derivada_v(Ij1) # Derivada da função sigmóide na camada oculta
    gradiente_local_oculta = derivada_saida_camada_oculta*((ww2[:, 1:]) * gradiente_local_saida[0]) # GRADIENTE LOCAL DA CAMADA OCULTA

  
    # configurando as matrizes
    gradiente_local_oculta_novo = np.asmatrix(gradiente_local_oculta) #lendo como matrix
    gradiente_local_oculta_novo = np.reshape(gradiente_local_oculta, (gradiente_local_oculta.shape[1],gradiente_local_oculta.shape[0])) #estamos, basicamente, transpondo a matrix

    x_novo = np.asmatrix(x) #lendo como matrix

    ww1 = ww1 + (taxa_aprendizagem*gradiente_local_oculta_novo).dot(x_novo) #Calculamos o peso ajustado
    ww1 = np.asarray(ww1) #Voltamos a ler como array para fazer os outros cálculos
      
    Saida_treinamento[i] = saida_neuronio # Salvamos a saída do neurônio nesta variável "Saida_treinamento"

  Saida_treinamento_total.iloc[epoca, :] = Saida_treinamento # Salvamos a saída do treinamento em todas as épocas
  EQM_train.iloc[0, epoca] = (Erro_quadratico/numero_colunas_treinamento) #Agora considerando todos os N momentos, ou seja, os N pares de entrada e saída que eu tenho no banco da dados. Esta é a função custo, o nosso erro quadrático médio, a que queremos minimzar durante o treinamento.

  return ww1,ww2

"""**Gerar relatório**"""

#DEFINIDO A FUNÇÃO QUE IRÁ COMPUTAR A MATRIZ DE CONFUSÃO E AS MÉTRICAS

def gerar_relatorio(Saida_desejada,Saida_obtida):


  Saida_desejada_new = pd.melt(Saida_desejada) # "Derretemos" o vetor para ele ficar no formato que queremos (várias linhas e 2 colunas)
  Saida_desejada_new.columns = ['Par_entrada_saida', 'Classe'] #renomeando as colunas
  Saida_obtida_new = pd.melt(Saida_obtida) # "Derretemos" o vetor para ele ficar no formato que queremos (várias linhas e 2 colunas)
  Saida_obtida_new.columns = ['variable', 'Saida_obtida'] #renomeando as colunas

  # Arredondando as saídas - Aqui arredondados para 0 ou 1, conforme o valor da saída serja menor ou maior que 0.5

  for i in range (Saida_obtida_new['Saida_obtida'].count()):

    if ((Saida_obtida_new['Saida_obtida'].loc[i]) <= 0.5):
      Saida_obtida_new['Saida_obtida'].loc[i] = 0
    else:
      Saida_obtida_new['Saida_obtida'].loc[i] = 1

  # Criando vetores para serem usados pela matriz de confusão

  Saida_obtida_array = np.array(Saida_obtida_new) #Criando o vetor
  Saida_obtida_array = Saida_obtida_array.astype(int) #Transformando os valores em inteiros

  Saida_desejada_array = np.array(Saida_desejada_new) #Criando o vetor

  #Criando a matriz de confusão
  cf_matrix = confusion_matrix(Saida_desejada_array[:,1], Saida_obtida_array[:,1])

  #Plotando a figura
  fig = plt.figure(figsize=(12,10))
  ax = sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, fmt='.2%', cmap='Blues')

  #Condigurações da figura
  ax.set_title('Matriz de Confusão - Mamografia\n\n');
  ax.set_xlabel('\nValores Previstos')
  ax.set_ylabel('Valores Reais');
  ax.xaxis.set_ticklabels(['False','True'])
  ax.yaxis.set_ticklabels(['False','True'])
  
  plt.show()


  #início das configurações para as métricas- Declaração de variáveis
  VP = 0
  VN = 0
  FP = 0
  FN = 0

  Precisao = 0
  Sensibilidade = 0
  F1 = 0 #media harmonica
  

  #Laço for que computa os valores de VP,VN,FP e FN
  for m in range(Saida_obtida_array[:,1].size): 
    if Saida_obtida_array[m][1] == 1:
      if Saida_desejada_array[m][1] == 1:
        VP = VP + 1
      else: 
        FP = FP + 1
    else:
      if Saida_desejada_array[m][1] == 0:
        VN = VN + 1
      else:
        FN = FN + 1

  try:
    Precisao = (VP/(VP+FP))
    print("Precisao > 0")
  except:
    print("O conjunto VP + FP = 0")
  try:
    Sensibilidade = (VP/(VP+FN))
  except:
    print("O conjunto VP + FN = 0")
  try:
    F1 = (2*Precisao*Sensibilidade)/(Precisao + Sensibilidade)
  except:
    print("Precisao + Sensibilidade = 0")
   
  #Acurácia
  acuracia = ((VP+VN)/(VP+VN+FP+FN))   

  print('....................... MATRIZ DE CONFUSÃO .......................')
  print(cf_matrix)
  print('\n')
  print('~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~. RELATÓRIO FINAL ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.')
  print('\n')
  print('-------- Valores --------')
  print('VP = {:.2f}\nVN = {:.2f}\nFP = {:.2f}\nFN = {:.2f}'.format(VP,VN,FP,FN))
  print('\n')
  print('-------- Métricas --------')
  print('Precisão = {:.2f}\nSensibilidade = {:.2f}\nF1 = {:.2f}\nAcurácia = {:.2f}'.format(Precisao,Sensibilidade,F1,acuracia))

"""**Predição com novos dados**"""

#DEFININDO A FUNÇÃO QUE IRÁ RODAR DADOS NOVOS EM UMA REDE NEURAL

def predicao(ww1,ww2,dados):

  for i in range(numero_colunas_teste-1): #Loop que faz iterações do tamanho da quantidade de colunas de treinamento, ou seja, número de pares de entrada no banco de dados

    saida_neuronio, x, Ij1, Yj1, y, Ij2 = neuronio(ww1,ww2,dados.iloc[:, i]) # Passamos os dados de entrada pelo neurônio artificial
    Saida_teste[i] = saida_neuronio # Salvamos a saída do neurônio nesta variável "Saida_treinamento"
    
  return Saida_teste

"""**Geraçõ de gráficos de treino**"""

# DEFININDO A FUNÇÃO QUE VAI GERAR OS GRÁFICOS DO TREINO

def gerar_graficos_de_treino(EQM_train,epoca,ERRO_validacao):

  # Tratando os dados para fazer os gráficos
  #Aqui nós "derretemos" os arrays para eles virarem um dataframe com colunas

  EQM_train_new = pd.melt(EQM_train) # "Derretemos" o vetor para ele ficar no formato que queremos (várias linhas e 2 colunas)
  EQM_train_new = EQM_train_new.truncate(after=epoca) # Truncamos o vetor que guarda os valores dos erros de treino pois ele está populado com zeros, e se o treinamento parar antes do número de épocas, esses zeros irão aparecer no gráfico.
  ERRO_validacao_new = pd.melt(ERRO_validacao) # "Derretemos" o vetor para ele ficar no formato que queremos (várias linhas e 2 colunas)
  ERRO_validacao_new = ERRO_validacao_new.truncate(after=epoca) # Truncamos o vetor que guarda os valores dos erros de validação pois ele está populado com zeros, e se o treinamento parar antes do número de épocas, esses zeros irão aparecer no gráfico.

  # --------------------------------------------------------- FIGURA 1 ---------------------------------------------------------
  fig = plt.figure(figsize=(30,7))

  ax1 = sns.lineplot(data=ERRO_validacao_new, 
                  x='variable', 
                  y='value',
                  )

  ax1.set_title('EVOLUÇÃO DO ERRO DE VALIDAÇÃO', fontsize=18)
  ax1.set_xlabel('ÉPOCAS', fontsize=15)
  ax1.set_ylabel('ERRO', fontsize=15)    
  ax1.set(xticks=np.arange(0,epocas+1,10))


  # --------------------------------------------------------- FIGURA 2 ---------------------------------------------------------
  fig = plt.figure(figsize=(30,7))

  #Gerando o gráfico
  ax2 = sns.lineplot(data=EQM_train_new, 
                  x='variable', 
                  y='value',
                  )

  ax2.set_title('EVOLUÇÃO DO ERRO DE TREINAMENTO', fontsize=18)
  ax2.set_xlabel('ÉPOCAS', fontsize=15)
  ax2.set_ylabel('ERRO QUADRÁTICO MÉDIO', fontsize=15)    
  ax2.set(xticks=np.arange(0,epocas+1,10))

  plt.show()

"""# Célula principal: É aqui que iremos rodar as épocas de treinamento e chamar todas as funções definidas anteriormente"""

criterio_valid = 0
criterio_treino = True


#--------------------------------- ETAPA DE TREINAMENTO --------------------------------
for epoca in range (epocas): 
  
  #ETAPA DE AJUSTE DE PESOS
  ww1,ww2 = ajuste_pesos(epoca,ww1,ww2)
  #print(ww1) #descomentar para acompanhar a mudança dos pesos
  #print(ww2) #descomentar para acompanhar a mudança dos pesos

  #ETAPA DE VALIDAÇÃO CRUZADA
  criterio_treino, criterio_valid = validacao_cruzada(epoca,criterio_treino,criterio_valid)

  if ((criterio_treino == True) and (criterio_valid > 3)): # NOTE: É essa linha que faz, basicamente, a função da validação cruzada.
    print('Critério de validação atingido. O treinamento foi parado.')
    break

#Vamos ver como se comportam os gráficos de treinamento
print(' .~.~.~.~.~.~. Gerando os gráficos de treinamento .~.~.~.~.~.~.')
print('\n')
gerar_graficos_de_treino(EQM_train,epoca,ERRO_validacao)

#Vamos gerar o relatório das métricas
print(' .~.~.~.~.~.~. Gerando as métricas .~.~.~.~.~.~.')

gerar_relatorio(Saida_desejada_treinamento,Saida_treinamento)

"""## Vamos rodar a base de dados de teste, computar as métricas e os pesos"""

#AGORA VAMOS AFZER A PREVISÃO NA BASE DE TESTE

saida_do_teste = predicao(ww1,ww2,Entrada_teste)

gerar_relatorio(Saida_desejada_teste,saida_do_teste)

"""**Analisando os pesos antes e depois do treinamento**"""

print(' ----------- Computando os pesos antes do treinamento ----------- \nw1:')
print(w1)
print('\nw2:')
print(w2)
print('\n')
print(' ----------- Computando os pesos depois do treinamento ----------- \nww1:')
print(ww1)
print('\nww2:')
print(ww2)